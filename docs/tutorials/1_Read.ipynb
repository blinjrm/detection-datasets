{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038b0b47-b813-499c-8ee4-440345548c17",
   "metadata": {},
   "source": [
    "# 1. Load from disk and upload to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb830fe-2727-4f75-a35f-e8f120a0146d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fffb4-991a-4be9-8d02-2360a315fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from detection_datasets import DetectionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e2185-f47d-4161-89c0-5b4a3b2410b1",
   "metadata": {},
   "source": [
    "## Download the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b1ba3-7a5e-4089-af7b-71b8fc2bb6fb",
   "metadata": {},
   "source": [
    "The files (images and annotations) are stored in S3, and the links for downloading them are provided in a [GitHub repository](https://github.com/cvdfoundation/fashionpedia).\n",
    "\n",
    "\n",
    "The dataset is formatted in the [COCO format](https://cocodataset.org/#format-data).\n",
    "\n",
    "Link for the `train` images:  \n",
    "https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip    \n",
    "Link for the `validation` images:   \n",
    "https://s3.amazonaws.com/ifashionist-dataset/images/val_test2020.zip   \n",
    "\n",
    "Link for the `train` annotations:   \n",
    "https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_train2020.json    \n",
    "Link for the `validation` annotations:   \n",
    "https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_val2020.json    \n",
    "\n",
    "You may notice the the `test` split is absent: this is because the dataset was part of a Kaggle competition, where the submission are evaluate on a holdout test data that is not public. \n",
    "See notebook 2. to see how to create your custom splits nevertheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31917263-790b-4763-a052-a8ce621b39d6",
   "metadata": {},
   "source": [
    "Let's first define some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eeabd5-2c14-4899-8b82-92e0d473de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from S3\n",
    "RAW_TRAIN_IMAGES = 'https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip'\n",
    "RAW_VAL_IMAGES = 'https://s3.amazonaws.com/ifashionist-dataset/images/val_test2020.zip'\n",
    "RAW_TRAIN_ANNOTATIONS = 'https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_train2020.json'\n",
    "RAW_VAL_ANNOTATIONS = 'https://s3.amazonaws.com/ifashionist-dataset/annotations/instances_attributes_val2020.json'\n",
    "\n",
    "# to local disk\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "TRAIN_ANNOTATIONS = 'train.json'\n",
    "VAL_ANNOTATIONS = 'val.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7a04c-accb-4eba-b72d-cf1897164e5f",
   "metadata": {},
   "source": [
    "And now download the images and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998795a3-1521-461a-8047-c594fad797c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(url, target):\n",
    "    \"\"\"Download image and annotations.\"\"\"\n",
    "    \n",
    "    # Images\n",
    "    if url.split('.')[-1] == 'zip':\n",
    "        path, _ = urllib.request.urlretrieve(url=url)\n",
    "        with zipfile.ZipFile(path, \"r\") as f:\n",
    "            f.extractall(target)\n",
    "            \n",
    "        os.remove(path)\n",
    "    \n",
    "    # Annotations\n",
    "    else:\n",
    "        urllib.request.urlretrieve(url=url, filename=target)\n",
    "\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "download(url=RAW_TRAIN_ANNOTATIONS, target=os.path.join(DATA_DIR, TRAIN_ANNOTATIONS))\n",
    "download(url=RAW_VAL_ANNOTATIONS, target=os.path.join(DATA_DIR, VAL_ANNOTATIONS))\n",
    "\n",
    "download(url=RAW_TRAIN_IMAGES, target=DATA_DIR)\n",
    "download(url=RAW_VAL_IMAGES, target=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c01ce1-de18-42be-a3ef-7cfb20bc9629",
   "metadata": {},
   "source": [
    "Here are the files and directories we have just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63baed84-6176-4668-a18a-ea1bb0ea79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282003bb-8404-416c-a057-5f6f2aab0155",
   "metadata": {},
   "source": [
    "Note the the validation images are in the 'test' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9c830-d74a-4e7f-8cfd-fd25f40c3f01",
   "metadata": {},
   "source": [
    "## Read the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe0b52-01c8-4913-ac7b-203735dae87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset_format': 'coco',                        # the format of the dataset on disk\n",
    "    'path': DATA_DIR,                                # where the dataset is located\n",
    "    'splits': {                                      # how to read the files\n",
    "        'train': (TRAIN_ANNOTATIONS, 'train'),       # name of the split (annotation file, images directory)\n",
    "        'val': (VAL_ANNOTATIONS, 'test'),               # the val directory get unziped in 'test'\n",
    "    },\n",
    "}\n",
    "\n",
    "dd = DetectionDataset().from_disk(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34bb13-cbab-4536-ba7d-a7ccd2568bb3",
   "metadata": {},
   "source": [
    "## Analyse the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975aaf0-f275-448a-8030-675b7448731a",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae8754-97ae-4756-aae6-789ff92e2f45",
   "metadata": {},
   "source": [
    "Internally the data is stored in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d9920-b8c3-421c-b580-44d2be00be97",
   "metadata": {},
   "source": [
    "It can viewed grouped by image (the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad3d2e-be71-4755-b11d-400cf44e949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.data      # This is the same as calling dd.get_data(index='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697b652-87bd-45f1-be60-9318cda9eac6",
   "metadata": {},
   "source": [
    "Or it can be viewed with one row for each annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec235349-b04d-4bb0-9a41-a5d355807c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.get_data(index='bbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276f2ed-e16e-4d3c-8dbd-fc5456565a11",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f3dd6-f463-4397-a414-ca5945292ceb",
   "metadata": {},
   "source": [
    "We can show an image an the annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374523f3-073c-464e-8793-15eee5b59583",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f2385-24a8-4424-bbde-2dcd03b33549",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e2520-be73-4eb9-8e87-8f244d461af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd.n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a76905-0fe0-4369-b085-d49c88f4caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.n_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd4aaa-7882-459b-8368-836afc6355b6",
   "metadata": {},
   "source": [
    "As mentionned earlier, there is no 'test' dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e85bf-2db9-4fbd-a1ea-a3b550adc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298e034-b5d7-43fa-bbbd-e993c00af076",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.split_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3261bb-6f13-4c25-9b19-ed3e5c7ad622",
   "metadata": {},
   "source": [
    "We also see that >97.5% of the images belong to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cc44e-c9ff-4d0e-b680-c0fb1cb6909e",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2832d6-3382-4b84-b67a-fd93e0b63d67",
   "metadata": {},
   "source": [
    "There are 46 categories in this dataset, we can get the full list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475e5e0-b08a-4834-ae50-b29959c0a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eced00-73a8-4b9b-8280-f3dd3ec312e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.category_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4378d5-b6b5-4e82-9706-2f5dc63ccaaa",
   "metadata": {},
   "source": [
    "Let's also show the categories with their ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa09042-e575-4797-9475-e03620d6449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7588d5-0106-4ab0-ad4a-ee58dae7b34c",
   "metadata": {},
   "source": [
    "## Upload to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3cb7d-4b6f-48e9-a69a-fbe6c127a0a2",
   "metadata": {},
   "source": [
    "We can now upload the dataset to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d3555-d885-43a0-828c-5ca3c27562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_hub(dataset_name='fashionpedia', repo_name='detection-datasets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
