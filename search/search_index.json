{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to detection-dataset # This site contains the project documentation for the detection-dataset project that makes it easy to convert datasets between different formats for object detection. Table Of Contents # The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework and consists of four separate parts: Tutorials How-to guides Reference Explanation Quickly find what you're looking for depending on your use case by looking at the different pages.","title":"Detection-dataset"},{"location":"#welcome-to-detection-dataset","text":"This site contains the project documentation for the detection-dataset project that makes it easy to convert datasets between different formats for object detection.","title":"Welcome to detection-dataset"},{"location":"#table-of-contents","text":"The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework and consists of four separate parts: Tutorials How-to guides Reference Explanation Quickly find what you're looking for depending on your use case by looking at the different pages.","title":"Table Of Contents"},{"location":"explanation/","text":"This part of the project documentation focuses on an understanding-oriented approach. You'll get a chance to read about the background of the project, as well as reasoning about how it was implemented.","title":"Explanation"},{"location":"how-to-guides/","text":"This part of the project documentation focuses on a problem-oriented approach. You'll tackle common tasks that you might have, with the help of the code provided in this project.","title":"How-to guides"},{"location":"tutorials/","text":"This part of the project documentation focuses on a learning-oriented approach. You'll learn how to get started with the code in this project.","title":"Tutorials"},{"location":"reference/","text":"API reference # The librairy exposes two main, user-facing classes to facilitate conversion, whether between complete formats or simply between bounding boxes format. Converter : converts datasets between different formats Bbox : converts bounding boxes between different formats. There are also some internal components that are exposed for advanced users: Readers","title":"Reference"},{"location":"reference/#api-reference","text":"The librairy exposes two main, user-facing classes to facilitate conversion, whether between complete formats or simply between bounding boxes format. Converter : converts datasets between different formats Bbox : converts bounding boxes between different formats. There are also some internal components that are exposed for advanced users: Readers","title":"API reference"},{"location":"reference/bbox/","text":"Class for manipulating bounding boxes. All bounding boxes are converted to internally the VOC format: [xmin, ymin, xmax, ymax], and can be exported to the VOC, COCO and YOLO formats. Source code in detection_dataset/bbox.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class Bbox : \"\"\"Class for manipulating bounding boxes. All bounding boxes are converted to internally the VOC format: [xmin, ymin, xmax, ymax], and can be exported to the VOC, COCO and YOLO formats. \"\"\" def __init__ ( self , bbox : list [ float ], width : float , height : float ) -> None : self . bbox = bbox self . width = width self . height = height self . _validate_bbox () @classmethod def from_voc ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Keep the bbox in VOC format: xmin, ymin, xmax, ymax.\"\"\" return Bbox ( bbox , width , height ) @classmethod def from_coco ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from COCO format: xmin, ymin, w, h.\"\"\" bbox = [ bbox [ 0 ], bbox [ 1 ], bbox [ 0 ] + bbox [ 2 ], bbox [ 1 ] + bbox [ 3 ]] return Bbox ( bbox , width , height ) @classmethod def from_yolo ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from YOLO format: relative xc, yc, w, h.\"\"\" assert bbox [ 0 ] < 1 and bbox [ 1 ] < 1 and bbox [ 2 ] < 1 and bbox [ 3 ] < 1 , \"yolo bbox must be relative\" bbox = [ bbox [ 0 ] - bbox [ 2 ] / 2 , bbox [ 1 ] - bbox [ 3 ] / 2 , bbox [ 0 ] + bbox [ 2 ] / 2 , bbox [ 1 ] + bbox [ 3 ] / 2 ] bbox = [ bbox [ 0 ] * width , bbox [ 1 ] * height , bbox [ 2 ] * width , bbox [ 3 ] * height ] return Bbox ( bbox , width , height ) def to_voc ( self ) -> list [ float ]: \"\"\"Bbox is already in VOC format internally.\"\"\" return self . bbox def to_coco ( self ) -> list [ float ]: \"\"\"Convert the bbox to COCO format: xmin, ymin, w, h.\"\"\" return [ self . bbox [ 0 ], self . bbox [ 1 ], self . bbox [ 2 ] - self . bbox [ 0 ], self . bbox [ 3 ] - self . bbox [ 1 ]] def to_yolo ( self ) -> list [ float ]: \"\"\"Convert the bbox to YOLO format: relative xc, yc, w, h.\"\"\" bbox = self . to_coco () bbox = [ bbox [ 0 ] / self . width , bbox [ 1 ] / self . height , bbox [ 2 ] / self . width , bbox [ 3 ] / self . height ] return bbox def _validate_bbox ( self ) -> None : \"\"\"Asserts that the bbox to the correct size.\"\"\" assert self . bbox [ 2 ] >= self . bbox [ 0 ] and self . bbox [ 3 ] >= self . bbox [ 1 ], \"bbox must be a rectangle\" assert self . bbox [ 2 ] <= self . width and self . bbox [ 3 ] <= self . height , \"bbox must be inside the image\" from_coco ( bbox , width , height ) classmethod # Convert the bbox from COCO format: xmin, ymin, w, h. Source code in detection_dataset/bbox.py 23 24 25 26 27 28 @classmethod def from_coco ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from COCO format: xmin, ymin, w, h.\"\"\" bbox = [ bbox [ 0 ], bbox [ 1 ], bbox [ 0 ] + bbox [ 2 ], bbox [ 1 ] + bbox [ 3 ]] return Bbox ( bbox , width , height ) from_voc ( bbox , width , height ) classmethod # Keep the bbox in VOC format: xmin, ymin, xmax, ymax. Source code in detection_dataset/bbox.py 17 18 19 20 21 @classmethod def from_voc ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Keep the bbox in VOC format: xmin, ymin, xmax, ymax.\"\"\" return Bbox ( bbox , width , height ) from_yolo ( bbox , width , height ) classmethod # Convert the bbox from YOLO format: relative xc, yc, w, h. Source code in detection_dataset/bbox.py 30 31 32 33 34 35 36 37 38 @classmethod def from_yolo ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from YOLO format: relative xc, yc, w, h.\"\"\" assert bbox [ 0 ] < 1 and bbox [ 1 ] < 1 and bbox [ 2 ] < 1 and bbox [ 3 ] < 1 , \"yolo bbox must be relative\" bbox = [ bbox [ 0 ] - bbox [ 2 ] / 2 , bbox [ 1 ] - bbox [ 3 ] / 2 , bbox [ 0 ] + bbox [ 2 ] / 2 , bbox [ 1 ] + bbox [ 3 ] / 2 ] bbox = [ bbox [ 0 ] * width , bbox [ 1 ] * height , bbox [ 2 ] * width , bbox [ 3 ] * height ] return Bbox ( bbox , width , height ) to_coco () # Convert the bbox to COCO format: xmin, ymin, w, h. Source code in detection_dataset/bbox.py 45 46 47 48 def to_coco ( self ) -> list [ float ]: \"\"\"Convert the bbox to COCO format: xmin, ymin, w, h.\"\"\" return [ self . bbox [ 0 ], self . bbox [ 1 ], self . bbox [ 2 ] - self . bbox [ 0 ], self . bbox [ 3 ] - self . bbox [ 1 ]] to_voc () # Bbox is already in VOC format internally. Source code in detection_dataset/bbox.py 40 41 42 43 def to_voc ( self ) -> list [ float ]: \"\"\"Bbox is already in VOC format internally.\"\"\" return self . bbox to_yolo () # Convert the bbox to YOLO format: relative xc, yc, w, h. Source code in detection_dataset/bbox.py 50 51 52 53 54 55 def to_yolo ( self ) -> list [ float ]: \"\"\"Convert the bbox to YOLO format: relative xc, yc, w, h.\"\"\" bbox = self . to_coco () bbox = [ bbox [ 0 ] / self . width , bbox [ 1 ] / self . height , bbox [ 2 ] / self . width , bbox [ 3 ] / self . height ] return bbox","title":"Bbox"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.from_coco","text":"Convert the bbox from COCO format: xmin, ymin, w, h. Source code in detection_dataset/bbox.py 23 24 25 26 27 28 @classmethod def from_coco ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from COCO format: xmin, ymin, w, h.\"\"\" bbox = [ bbox [ 0 ], bbox [ 1 ], bbox [ 0 ] + bbox [ 2 ], bbox [ 1 ] + bbox [ 3 ]] return Bbox ( bbox , width , height )","title":"from_coco()"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.from_voc","text":"Keep the bbox in VOC format: xmin, ymin, xmax, ymax. Source code in detection_dataset/bbox.py 17 18 19 20 21 @classmethod def from_voc ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Keep the bbox in VOC format: xmin, ymin, xmax, ymax.\"\"\" return Bbox ( bbox , width , height )","title":"from_voc()"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.from_yolo","text":"Convert the bbox from YOLO format: relative xc, yc, w, h. Source code in detection_dataset/bbox.py 30 31 32 33 34 35 36 37 38 @classmethod def from_yolo ( cls , bbox : list [ float ], width : float , height : float ) -> Bbox : \"\"\"Convert the bbox from YOLO format: relative xc, yc, w, h.\"\"\" assert bbox [ 0 ] < 1 and bbox [ 1 ] < 1 and bbox [ 2 ] < 1 and bbox [ 3 ] < 1 , \"yolo bbox must be relative\" bbox = [ bbox [ 0 ] - bbox [ 2 ] / 2 , bbox [ 1 ] - bbox [ 3 ] / 2 , bbox [ 0 ] + bbox [ 2 ] / 2 , bbox [ 1 ] + bbox [ 3 ] / 2 ] bbox = [ bbox [ 0 ] * width , bbox [ 1 ] * height , bbox [ 2 ] * width , bbox [ 3 ] * height ] return Bbox ( bbox , width , height )","title":"from_yolo()"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.to_coco","text":"Convert the bbox to COCO format: xmin, ymin, w, h. Source code in detection_dataset/bbox.py 45 46 47 48 def to_coco ( self ) -> list [ float ]: \"\"\"Convert the bbox to COCO format: xmin, ymin, w, h.\"\"\" return [ self . bbox [ 0 ], self . bbox [ 1 ], self . bbox [ 2 ] - self . bbox [ 0 ], self . bbox [ 3 ] - self . bbox [ 1 ]]","title":"to_coco()"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.to_voc","text":"Bbox is already in VOC format internally. Source code in detection_dataset/bbox.py 40 41 42 43 def to_voc ( self ) -> list [ float ]: \"\"\"Bbox is already in VOC format internally.\"\"\" return self . bbox","title":"to_voc()"},{"location":"reference/bbox/#detection_dataset.bbox.Bbox.to_yolo","text":"Convert the bbox to YOLO format: relative xc, yc, w, h. Source code in detection_dataset/bbox.py 50 51 52 53 54 55 def to_yolo ( self ) -> list [ float ]: \"\"\"Convert the bbox to YOLO format: relative xc, yc, w, h.\"\"\" bbox = self . to_coco () bbox = [ bbox [ 0 ] / self . width , bbox [ 1 ] / self . height , bbox [ 2 ] / self . width , bbox [ 3 ] / self . height ] return bbox","title":"to_yolo()"},{"location":"reference/converter/","text":"Convert a dataset from one format to another. Their are 3 steps to convert a dataset: - Read the existing dataset, specifying the format - Optionally transform the dataset - Write the dataset to a new format, specifying the destination Source code in detection_dataset/convert.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class Converter : \"\"\"Convert a dataset from one format to another. Their are 3 steps to convert a dataset: - Read the existing dataset, specifying the format - Optionally transform the dataset - Write the dataset to a new format, specifying the destination \"\"\" def __init__ ( self ) -> None : \"\"\"Initialize the converter.\"\"\" self . _dataset = Dataset () self . category_mapping = None def read ( self , dataset_format : str , path : str , ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Reads the dataset. This is a factory method that can read the dataset from different format. Args: dataset_format: Format of the dataset. Currently supported formats: - \"coco\": COCO format path: Path to the dataset on the local filesystem. **kwargs: Keyword arguments specific to the dataset_format. \"\"\" config = {} config [ \"path\" ] = path reader = reader_factory . get ( dataset_format , ** config ) data = reader . load ( ** kwargs ) self . _dataset . concat ( data ) def transform ( self , category_mapping : Optional [ pd . DataFrame ] = None , n_images : Optional [ int ] = None , splits : Optional [ Tuple [ Union [ int , float ]]] = None , ) -> None : \"\"\"Transforms the dataset. 3 types of transformations can be applied to the dataset: - Map existing categories to new categories - Reduce the number of images - create new (train, val, test) splits Args: category_mapping (optional): A DataFrame mapping original categories to new categories. Defaults to None. n_images (optional): Number of images to include in the dataset. Respects the proportion of images in each split. Defaults to None. splits: Iterable containing the proportion of images to include in the train, val and test splits. The sum of the values in the iterable must be equal to 1. The original splits will be overwritten. Defaults to None. \"\"\" if category_mapping is not None : self . category_mapping = category_mapping self . _dataset . map_categories ( category_mapping ) if splits : self . _dataset . split ( splits ) if n_images : self . _dataset . limit_images ( n_images ) def write ( self , dataset_format : str , name : str , destinations : Union [ str , List [ str ]], ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Writes the dataset. This is a factory method that can write the dataset: - In a given format (e.g. COCO, MMDET, YOLO) - To a given destination (e.g. local directory, W&B artifacts) Args: dataset_format: Format of the dataset. Currently supported formats: - \"yolo\": YOLO format - \"mmdet\": MMDET internal format, see: https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#reorganize-new-data-format-to-middle-format name: Name of the dataset. destinations: Where to write the dataset. Currently supported destinations: - \"local_disk\": Local disk - \"wandb\": W&B artifacts **kwargs: Keyword arguments specific to the dataset_format. \"\"\" if not isinstance ( destinations , list ): destinations = [ destinations ] config = {} config [ \"dataset\" ] = self . _dataset config [ \"name\" ] = name config [ \"destinations\" ] = destinations if Destinations . LOCAL_DISK in destinations : if not kwargs . get ( \"path\" , None ): raise ValueError ( \"Path must be specified when writing to local filesystem.\" ) config [ \"path\" ] = kwargs [ \"path\" ] else : config [ \"path\" ] = DEFAULT_DATASET_DIR writer = writer_factory . get ( dataset_format , ** config ) writer . write () @property def dataset ( self ) -> Dataset : \"\"\"Access the class dataset. Returns: A dataset object containing the data. \"\"\" return self . _dataset @property def data ( self ) -> Dataset : \"\"\"Access the class data. Returns: A pandas DataFrame containing the dataset. \"\"\" return self . _dataset . data __init__ () # Initialize the converter. Source code in detection_dataset/convert.py 19 20 21 22 23 def __init__ ( self ) -> None : \"\"\"Initialize the converter.\"\"\" self . _dataset = Dataset () self . category_mapping = None data () property # Access the class data. Returns: Type Description Dataset A pandas DataFrame containing the dataset. Source code in detection_dataset/convert.py 135 136 137 138 139 140 141 142 143 @property def data ( self ) -> Dataset : \"\"\"Access the class data. Returns: A pandas DataFrame containing the dataset. \"\"\" return self . _dataset . data dataset () property # Access the class dataset. Returns: Type Description Dataset A dataset object containing the data. Source code in detection_dataset/convert.py 125 126 127 128 129 130 131 132 133 @property def dataset ( self ) -> Dataset : \"\"\"Access the class dataset. Returns: A dataset object containing the data. \"\"\" return self . _dataset read ( dataset_format , path , ** kwargs ) # Reads the dataset. This is a factory method that can read the dataset from different format. Parameters: Name Type Description Default dataset_format str Format of the dataset. Currently supported formats: - \"coco\": COCO format required path str Path to the dataset on the local filesystem. required **kwargs Dict [ str , str ] Keyword arguments specific to the dataset_format. {} Source code in detection_dataset/convert.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def read ( self , dataset_format : str , path : str , ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Reads the dataset. This is a factory method that can read the dataset from different format. Args: dataset_format: Format of the dataset. Currently supported formats: - \"coco\": COCO format path: Path to the dataset on the local filesystem. **kwargs: Keyword arguments specific to the dataset_format. \"\"\" config = {} config [ \"path\" ] = path reader = reader_factory . get ( dataset_format , ** config ) data = reader . load ( ** kwargs ) self . _dataset . concat ( data ) transform ( category_mapping = None , n_images = None , splits = None ) # Transforms the dataset. 3 types of transformations can be applied to the dataset: - Map existing categories to new categories - Reduce the number of images - create new (train, val, test) splits Parameters: Name Type Description Default category_mapping optional A DataFrame mapping original categories to new categories. Defaults to None. None n_images optional Number of images to include in the dataset. Respects the proportion of images in each split. Defaults to None. None splits Optional [ Tuple [ Union [ int , float ]]] Iterable containing the proportion of images to include in the train, val and test splits. The sum of the values in the iterable must be equal to 1. The original splits will be overwritten. Defaults to None. None Source code in detection_dataset/convert.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def transform ( self , category_mapping : Optional [ pd . DataFrame ] = None , n_images : Optional [ int ] = None , splits : Optional [ Tuple [ Union [ int , float ]]] = None , ) -> None : \"\"\"Transforms the dataset. 3 types of transformations can be applied to the dataset: - Map existing categories to new categories - Reduce the number of images - create new (train, val, test) splits Args: category_mapping (optional): A DataFrame mapping original categories to new categories. Defaults to None. n_images (optional): Number of images to include in the dataset. Respects the proportion of images in each split. Defaults to None. splits: Iterable containing the proportion of images to include in the train, val and test splits. The sum of the values in the iterable must be equal to 1. The original splits will be overwritten. Defaults to None. \"\"\" if category_mapping is not None : self . category_mapping = category_mapping self . _dataset . map_categories ( category_mapping ) if splits : self . _dataset . split ( splits ) if n_images : self . _dataset . limit_images ( n_images ) write ( dataset_format , name , destinations , ** kwargs ) # Writes the dataset. This is a factory method that can write the dataset: - In a given format (e.g. COCO, MMDET, YOLO) - To a given destination (e.g. local directory, W&B artifacts) Parameters: Name Type Description Default dataset_format str Format of the dataset. Currently supported formats: - \"yolo\": YOLO format - \"mmdet\": MMDET internal format, see: https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#reorganize-new-data-format-to-middle-format required name str Name of the dataset. required destinations Union [ str , List [ str ]] Where to write the dataset. Currently supported destinations: - \"local_disk\": Local disk - \"wandb\": W&B artifacts required **kwargs Dict [ str , str ] Keyword arguments specific to the dataset_format. {} Source code in detection_dataset/convert.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def write ( self , dataset_format : str , name : str , destinations : Union [ str , List [ str ]], ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Writes the dataset. This is a factory method that can write the dataset: - In a given format (e.g. COCO, MMDET, YOLO) - To a given destination (e.g. local directory, W&B artifacts) Args: dataset_format: Format of the dataset. Currently supported formats: - \"yolo\": YOLO format - \"mmdet\": MMDET internal format, see: https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#reorganize-new-data-format-to-middle-format name: Name of the dataset. destinations: Where to write the dataset. Currently supported destinations: - \"local_disk\": Local disk - \"wandb\": W&B artifacts **kwargs: Keyword arguments specific to the dataset_format. \"\"\" if not isinstance ( destinations , list ): destinations = [ destinations ] config = {} config [ \"dataset\" ] = self . _dataset config [ \"name\" ] = name config [ \"destinations\" ] = destinations if Destinations . LOCAL_DISK in destinations : if not kwargs . get ( \"path\" , None ): raise ValueError ( \"Path must be specified when writing to local filesystem.\" ) config [ \"path\" ] = kwargs [ \"path\" ] else : config [ \"path\" ] = DEFAULT_DATASET_DIR writer = writer_factory . get ( dataset_format , ** config ) writer . write ()","title":"Converter"},{"location":"reference/converter/#detection_dataset.convert.Converter.__init__","text":"Initialize the converter. Source code in detection_dataset/convert.py 19 20 21 22 23 def __init__ ( self ) -> None : \"\"\"Initialize the converter.\"\"\" self . _dataset = Dataset () self . category_mapping = None","title":"__init__()"},{"location":"reference/converter/#detection_dataset.convert.Converter.data","text":"Access the class data. Returns: Type Description Dataset A pandas DataFrame containing the dataset. Source code in detection_dataset/convert.py 135 136 137 138 139 140 141 142 143 @property def data ( self ) -> Dataset : \"\"\"Access the class data. Returns: A pandas DataFrame containing the dataset. \"\"\" return self . _dataset . data","title":"data()"},{"location":"reference/converter/#detection_dataset.convert.Converter.dataset","text":"Access the class dataset. Returns: Type Description Dataset A dataset object containing the data. Source code in detection_dataset/convert.py 125 126 127 128 129 130 131 132 133 @property def dataset ( self ) -> Dataset : \"\"\"Access the class dataset. Returns: A dataset object containing the data. \"\"\" return self . _dataset","title":"dataset()"},{"location":"reference/converter/#detection_dataset.convert.Converter.read","text":"Reads the dataset. This is a factory method that can read the dataset from different format. Parameters: Name Type Description Default dataset_format str Format of the dataset. Currently supported formats: - \"coco\": COCO format required path str Path to the dataset on the local filesystem. required **kwargs Dict [ str , str ] Keyword arguments specific to the dataset_format. {} Source code in detection_dataset/convert.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def read ( self , dataset_format : str , path : str , ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Reads the dataset. This is a factory method that can read the dataset from different format. Args: dataset_format: Format of the dataset. Currently supported formats: - \"coco\": COCO format path: Path to the dataset on the local filesystem. **kwargs: Keyword arguments specific to the dataset_format. \"\"\" config = {} config [ \"path\" ] = path reader = reader_factory . get ( dataset_format , ** config ) data = reader . load ( ** kwargs ) self . _dataset . concat ( data )","title":"read()"},{"location":"reference/converter/#detection_dataset.convert.Converter.transform","text":"Transforms the dataset. 3 types of transformations can be applied to the dataset: - Map existing categories to new categories - Reduce the number of images - create new (train, val, test) splits Parameters: Name Type Description Default category_mapping optional A DataFrame mapping original categories to new categories. Defaults to None. None n_images optional Number of images to include in the dataset. Respects the proportion of images in each split. Defaults to None. None splits Optional [ Tuple [ Union [ int , float ]]] Iterable containing the proportion of images to include in the train, val and test splits. The sum of the values in the iterable must be equal to 1. The original splits will be overwritten. Defaults to None. None Source code in detection_dataset/convert.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def transform ( self , category_mapping : Optional [ pd . DataFrame ] = None , n_images : Optional [ int ] = None , splits : Optional [ Tuple [ Union [ int , float ]]] = None , ) -> None : \"\"\"Transforms the dataset. 3 types of transformations can be applied to the dataset: - Map existing categories to new categories - Reduce the number of images - create new (train, val, test) splits Args: category_mapping (optional): A DataFrame mapping original categories to new categories. Defaults to None. n_images (optional): Number of images to include in the dataset. Respects the proportion of images in each split. Defaults to None. splits: Iterable containing the proportion of images to include in the train, val and test splits. The sum of the values in the iterable must be equal to 1. The original splits will be overwritten. Defaults to None. \"\"\" if category_mapping is not None : self . category_mapping = category_mapping self . _dataset . map_categories ( category_mapping ) if splits : self . _dataset . split ( splits ) if n_images : self . _dataset . limit_images ( n_images )","title":"transform()"},{"location":"reference/converter/#detection_dataset.convert.Converter.write","text":"Writes the dataset. This is a factory method that can write the dataset: - In a given format (e.g. COCO, MMDET, YOLO) - To a given destination (e.g. local directory, W&B artifacts) Parameters: Name Type Description Default dataset_format str Format of the dataset. Currently supported formats: - \"yolo\": YOLO format - \"mmdet\": MMDET internal format, see: https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#reorganize-new-data-format-to-middle-format required name str Name of the dataset. required destinations Union [ str , List [ str ]] Where to write the dataset. Currently supported destinations: - \"local_disk\": Local disk - \"wandb\": W&B artifacts required **kwargs Dict [ str , str ] Keyword arguments specific to the dataset_format. {} Source code in detection_dataset/convert.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def write ( self , dataset_format : str , name : str , destinations : Union [ str , List [ str ]], ** kwargs : Dict [ str , str ], ) -> None : \"\"\"Writes the dataset. This is a factory method that can write the dataset: - In a given format (e.g. COCO, MMDET, YOLO) - To a given destination (e.g. local directory, W&B artifacts) Args: dataset_format: Format of the dataset. Currently supported formats: - \"yolo\": YOLO format - \"mmdet\": MMDET internal format, see: https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#reorganize-new-data-format-to-middle-format name: Name of the dataset. destinations: Where to write the dataset. Currently supported destinations: - \"local_disk\": Local disk - \"wandb\": W&B artifacts **kwargs: Keyword arguments specific to the dataset_format. \"\"\" if not isinstance ( destinations , list ): destinations = [ destinations ] config = {} config [ \"dataset\" ] = self . _dataset config [ \"name\" ] = name config [ \"destinations\" ] = destinations if Destinations . LOCAL_DISK in destinations : if not kwargs . get ( \"path\" , None ): raise ValueError ( \"Path must be specified when writing to local filesystem.\" ) config [ \"path\" ] = kwargs [ \"path\" ] else : config [ \"path\" ] = DEFAULT_DATASET_DIR writer = writer_factory . get ( dataset_format , ** config ) writer . write ()","title":"write()"},{"location":"reference/readers/","text":"Bases: ABC Source code in detection_dataset/readers/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class BaseReader ( ABC ): def __init__ ( self , path : str ) -> None : \"\"\"Base class for loading datasets in memory. Args: path: Path to the dataset \"\"\" self . path = path @abstractmethod def load ( self ) -> Dataset : \"\"\"Load a dataset. Returns: A Dataset instance containing the data loaded. \"\"\" __init__ ( path ) # Base class for loading datasets in memory. Parameters: Name Type Description Default path str Path to the dataset required Source code in detection_dataset/readers/base.py 7 8 9 10 11 12 13 14 def __init__ ( self , path : str ) -> None : \"\"\"Base class for loading datasets in memory. Args: path: Path to the dataset \"\"\" self . path = path load () abstractmethod # Load a dataset. Returns: Type Description Dataset A Dataset instance containing the data loaded. Source code in detection_dataset/readers/base.py 16 17 18 19 20 21 22 @abstractmethod def load ( self ) -> Dataset : \"\"\"Load a dataset. Returns: A Dataset instance containing the data loaded. \"\"\" Bases: BaseReader Source code in detection_dataset/readers/coco.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class CocoReader ( BaseReader ): def __init__ ( self , path : str ) -> None : super () . __init__ ( path ) def load ( self , splits : Dict [ str , Tuple [ str , str ]], ** kwargs ) -> Dataset : annotation_dataframes = [] for split , ( annotation_file , images_dir ) in splits . items (): images_path_prefix = os . path . join ( self . path , images_dir ) json = self . _read_json ( self . path , annotation_file ) annotation_dataframe = self . _read_images_annotations ( json ) annotation_dataframe [ \"image_path\" ] = annotation_dataframe [ \"image_name\" ] . apply ( lambda x : os . path . join ( images_path_prefix , x ) ) annotation_dataframe [ \"split\" ] = split annotation_dataframes . append ( annotation_dataframe ) annotation_by_bbox = pd . concat ( annotation_dataframes , axis = 0 , ignore_index = True ) annotation_by_bbox [ \"bbox\" ] = [ Bbox . from_coco ( row . bbox , row . width , row . height ) for _ , row in annotation_by_bbox . iterrows () ] return annotation_by_bbox . set_index ([ \"image_id\" , \"bbox_id\" ]) @staticmethod def _read_json ( path : str , file : str ) -> json : path_to_file = os . path . join ( path , file ) with open ( path_to_file ) as f : return json . load ( f ) @staticmethod def _read_images_annotations ( json_data : json ) -> pd . DataFrame : annotations = pd . DataFrame ( json_data [ \"annotations\" ]) annotations = annotations . drop ( columns = [ \"segmentation\" , \"iscrowd\" , \"attribute_ids\" ], errors = \"ignore\" ) annotations = annotations . rename ( columns = { \"id\" : \"bbox_id\" }) images = pd . DataFrame ( json_data [ \"images\" ]) images = images . drop ( columns = [ \"license\" , \"time_captured\" , \"original_url\" , \"isstatic\" , \"kaggle_id\" ], errors = \"ignore\" ) images = images . rename ( columns = { \"id\" : \"image_id\" , \"file_name\" : \"image_name\" }) categories = pd . DataFrame ( json_data [ \"categories\" ]) categories = categories . drop ( columns = [ \"level\" , \"taxonomy_id\" ], errors = \"ignore\" ) categories = categories . rename ( columns = { \"id\" : \"category_id\" , \"name\" : \"category\" }) data = pd . merge ( annotations , images , on = \"image_id\" , how = \"left\" ) data = pd . merge ( data , categories , on = \"category_id\" , how = \"left\" ) return data","title":"Readers"},{"location":"reference/readers/#detection_dataset.readers.base.BaseReader.__init__","text":"Base class for loading datasets in memory. Parameters: Name Type Description Default path str Path to the dataset required Source code in detection_dataset/readers/base.py 7 8 9 10 11 12 13 14 def __init__ ( self , path : str ) -> None : \"\"\"Base class for loading datasets in memory. Args: path: Path to the dataset \"\"\" self . path = path","title":"__init__()"},{"location":"reference/readers/#detection_dataset.readers.base.BaseReader.load","text":"Load a dataset. Returns: Type Description Dataset A Dataset instance containing the data loaded. Source code in detection_dataset/readers/base.py 16 17 18 19 20 21 22 @abstractmethod def load ( self ) -> Dataset : \"\"\"Load a dataset. Returns: A Dataset instance containing the data loaded. \"\"\" Bases: BaseReader Source code in detection_dataset/readers/coco.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class CocoReader ( BaseReader ): def __init__ ( self , path : str ) -> None : super () . __init__ ( path ) def load ( self , splits : Dict [ str , Tuple [ str , str ]], ** kwargs ) -> Dataset : annotation_dataframes = [] for split , ( annotation_file , images_dir ) in splits . items (): images_path_prefix = os . path . join ( self . path , images_dir ) json = self . _read_json ( self . path , annotation_file ) annotation_dataframe = self . _read_images_annotations ( json ) annotation_dataframe [ \"image_path\" ] = annotation_dataframe [ \"image_name\" ] . apply ( lambda x : os . path . join ( images_path_prefix , x ) ) annotation_dataframe [ \"split\" ] = split annotation_dataframes . append ( annotation_dataframe ) annotation_by_bbox = pd . concat ( annotation_dataframes , axis = 0 , ignore_index = True ) annotation_by_bbox [ \"bbox\" ] = [ Bbox . from_coco ( row . bbox , row . width , row . height ) for _ , row in annotation_by_bbox . iterrows () ] return annotation_by_bbox . set_index ([ \"image_id\" , \"bbox_id\" ]) @staticmethod def _read_json ( path : str , file : str ) -> json : path_to_file = os . path . join ( path , file ) with open ( path_to_file ) as f : return json . load ( f ) @staticmethod def _read_images_annotations ( json_data : json ) -> pd . DataFrame : annotations = pd . DataFrame ( json_data [ \"annotations\" ]) annotations = annotations . drop ( columns = [ \"segmentation\" , \"iscrowd\" , \"attribute_ids\" ], errors = \"ignore\" ) annotations = annotations . rename ( columns = { \"id\" : \"bbox_id\" }) images = pd . DataFrame ( json_data [ \"images\" ]) images = images . drop ( columns = [ \"license\" , \"time_captured\" , \"original_url\" , \"isstatic\" , \"kaggle_id\" ], errors = \"ignore\" ) images = images . rename ( columns = { \"id\" : \"image_id\" , \"file_name\" : \"image_name\" }) categories = pd . DataFrame ( json_data [ \"categories\" ]) categories = categories . drop ( columns = [ \"level\" , \"taxonomy_id\" ], errors = \"ignore\" ) categories = categories . rename ( columns = { \"id\" : \"category_id\" , \"name\" : \"category\" }) data = pd . merge ( annotations , images , on = \"image_id\" , how = \"left\" ) data = pd . merge ( data , categories , on = \"category_id\" , how = \"left\" ) return data","title":"load()"}]}